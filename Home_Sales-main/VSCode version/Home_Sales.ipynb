{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_KW73O2e3dw","outputId":"fa5fd2b3-e2de-491b-ee1c-405317ba7ebc"},"outputs":[{"ename":"ValueError","evalue":"Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation).","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Import findspark and initialize. \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfindspark\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m findspark\u001b[39m.\u001b[39;49minit()\n","File \u001b[1;32mc:\\Users\\tharu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\findspark.py:143\u001b[0m, in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Make pyspark importable.\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \n\u001b[0;32m    123\u001b[0m \u001b[39mSets environment variables and adds dependencies to sys.path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39m    configure and import pyspark.\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m spark_home:\n\u001b[1;32m--> 143\u001b[0m     spark_home \u001b[39m=\u001b[39m find()\n\u001b[0;32m    145\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m python_path:\n\u001b[0;32m    146\u001b[0m     python_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mPYSPARK_PYTHON\u001b[39m\u001b[39m\"\u001b[39m, sys\u001b[39m.\u001b[39mexecutable)\n","File \u001b[1;32mc:\\Users\\tharu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\findspark.py:46\u001b[0m, in \u001b[0;36mfind\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m         spark_home \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(pyspark\u001b[39m.\u001b[39m\u001b[39m__file__\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m spark_home:\n\u001b[1;32m---> 46\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     47\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find Spark, make sure SPARK_HOME env is set\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     48\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m or Spark is in an expected location (e.g. from homebrew installation).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m     )\n\u001b[0;32m     51\u001b[0m \u001b[39mreturn\u001b[39;00m spark_home\n","\u001b[1;31mValueError\u001b[0m: Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation)."]}],"source":["# Import findspark and initialize. \n","import findspark\n","findspark.init()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2XbWNf1Te5fM"},"outputs":[],"source":["# Import packages\n","from pyspark.sql import SparkSession\n","import time\n","\n","# Create a SparkSession\n","spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wOJqxG_RPSwp"},"outputs":[],"source":["# 1. Read in the AWS S3 bucket into a DataFrame.\n","from pyspark import SparkFiles\n","url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n","\n","\n","#creating the dataframe \n","\n","spark.sparkContext.addFile(url)\n","home_df = spark.read.option('header', 'true').csv(SparkFiles.get(\"home_sales_revised.csv\"), inferSchema=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RoljcJ7WPpnm"},"outputs":[],"source":["# 2. Create a temporary view of the DataFrame.\n","\n","home_df.show()  #i wanted to see how the dataframe looked like\n","\n","home_df.createOrReplaceTempView('home_sales')  #the bootcamp spot document advised to use \"home_sales\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L6fkwOeOmqvq"},"outputs":[],"source":["# 3. What is the average price for a four bedroom house sold in each year rounded to two decimal places?\n","\n","avg_price_4b = \"\"\"\n","(SELECT date_built, ROUND(avg(price),2)\n","FROM home_sales\n","WHERE bedrooms==4 \n","GROUP BY date_built\n","ORDER BY date_built ASC)\n","\"\"\"\n","\n","spark.sql(avg_price_4b).show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8p_tUS8h8it"},"outputs":[],"source":["# 4. What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal places?\n","avg_price_3b_3b = \"\"\"\n","(SELECT date_built, ROUND(avg(price),2)\n","FROM home_sales\n","WHERE bedrooms==3 AND bathrooms==3\n","GROUP BY date_built\n","ORDER BY date_built ASC)\n","\"\"\"\n","\n","spark.sql(avg_price_3b_3b).show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y-Eytz64liDU"},"outputs":[],"source":["#  5. What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors,\n","# and are greater than or equal to 2,000 square feet rounded to two decimal places?\n","\n","\n","avg_price_3b_3b_2f = \"\"\"\n","(SELECT date_built, ROUND(avg(price),2)\n","FROM home_sales\n","WHERE bedrooms==3 AND bathrooms==3 AND floors==2 AND sqft_living>=2000\n","GROUP BY date_built\n","ORDER BY date_built ASC)\n","\"\"\"\n","\n","spark.sql(avg_price_3b_3b_2f).show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GUrfgOX1pCRd"},"outputs":[],"source":["# 6. What is the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than\n","# or equal to $350,000? Although this is a small dataset, determine the run time for this query.\n","\n","start_time = time.time()\n","\n","\n","avg_home350k = \"\"\"\n","(SELECT view, ROUND(avg(price),2) as average_home_price\n","FROM home_sales\n","WHERE price>=350000\n","GROUP BY view\n","ORDER BY view ASC)\n","\"\"\"\n","\n","spark.sql(avg_home350k).show()\n","\n","\n","print(\"--- %s seconds ---\" % (time.time() - start_time))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KAhk3ZD2tFy8"},"outputs":[],"source":["# 7. Cache the the temporary table home_sales.\n","spark.sql(\"cache table home_sales\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4opVhbvxtL-i"},"outputs":[],"source":["# 8. Check if the table is cached.\n","spark.catalog.isCached('home_sales')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5GnL46lwTSEk"},"outputs":[],"source":["# 9. Using the cached data, run the query that filters out the view ratings with average price \n","#  greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime.\n","\n","start_time = time.time()\n","\n","avg_home350k_cache = \"\"\"\n","(SELECT view, ROUND(avg(price),2) as average_home_price\n","FROM home_sales\n","WHERE price>=350000\n","GROUP BY view\n","ORDER BY view ASC)\n","\"\"\"\n","\n","spark.sql(avg_home350k_cache).show()\n","\n","print(\"--- %s seconds ---\" % (time.time() - start_time))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qm12WN9isHBR"},"outputs":[],"source":["# 10. Partition by the \"date_built\" field on the formatted parquet home sales data \n","home_df.write.parquet('parquet_homes', mode='overwrite')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AZ7BgY61sRqY"},"outputs":[],"source":["# 11. Read the formatted parquet data.\n","parquet_data=spark.read.parquet('parquet_homes')\n","#parquet_data   # wanted to view the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J6MJkHfvVcvh"},"outputs":[],"source":["# 12. Create a temporary table for the parquet data.\n","parquet_data.createOrReplaceTempView('parquet_homes_temp')   #i used parquet_data instead of home_sales because it made more sense"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G_Vhb52rU1Sn"},"outputs":[],"source":["# 13. Run the query that filters out the view ratings with average price of greater than or eqaul to $350,000 \n","# with the parquet DataFrame. Round your average to two decimal places. \n","# Determine the runtime and compare it to the cached version. \n","\n","start_time = time.time()\n","\n","\n","start_time = time.time()\n","\n","spark.sql(\"\"\"\n","(SELECT view, ROUND(avg(price),2)\n","FROM parquet_homes_temp\n","WHERE price>=350000\n","GROUP BY view\n","ORDER BY view ASC)\n","\"\"\").show()\n","\n","\n","print(\"--- %s seconds ---\" % (time.time() - start_time))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjjYzQGjtbq8"},"outputs":[],"source":["# 14. Uncache the home_sales temporary table.\n","spark.sql(\"uncache table home_sales\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sy9NBvO7tlmm"},"outputs":[],"source":["# 15. Check if the home_sales is no longer cached\n","\n","spark.catalog.isCached(\"home_sales\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CgsJEW9baZZQ"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"nteract":{"version":"0.28.0"}},"nbformat":4,"nbformat_minor":0}